{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L3_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1ToNTkjdmMW",
        "colab_type": "code",
        "outputId": "a0fb916a-cd82-4a8f-c6c5-e81e45b0cb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!cd transformers\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/43)\u001b[K\rremote: Counting objects:   4% (2/43)\u001b[K\rremote: Counting objects:   6% (3/43)\u001b[K\rremote: Counting objects:   9% (4/43)\u001b[K\rremote: Counting objects:  11% (5/43)\u001b[K\rremote: Counting objects:  13% (6/43)\u001b[K\rremote: Counting objects:  16% (7/43)\u001b[K\rremote: Counting objects:  18% (8/43)\u001b[K\rremote: Counting objects:  20% (9/43)\u001b[K\rremote: Counting objects:  23% (10/43)\u001b[K\rremote: Counting objects:  25% (11/43)\u001b[K\rremote: Counting objects:  27% (12/43)\u001b[K\rremote: Counting objects:  30% (13/43)\u001b[K\rremote: Counting objects:  32% (14/43)\u001b[K\rremote: Counting objects:  34% (15/43)\u001b[K\rremote: Counting objects:  37% (16/43)\u001b[K\rremote: Counting objects:  39% (17/43)\u001b[K\rremote: Counting objects:  41% (18/43)\u001b[K\rremote: Counting objects:  44% (19/43)\u001b[K\rremote: Counting objects:  46% (20/43)\u001b[K\rremote: Counting objects:  48% (21/43)\u001b[K\rremote: Counting objects:  51% (22/43)\u001b[K\rremote: Counting objects:  53% (23/43)\u001b[K\rremote: Counting objects:  55% (24/43)\u001b[K\rremote: Counting objects:  58% (25/43)\u001b[K\rremote: Counting objects:  60% (26/43)\u001b[K\rremote: Counting objects:  62% (27/43)\u001b[K\rremote: Counting objects:  65% (28/43)\u001b[K\rremote: Counting objects:  67% (29/43)\u001b[K\rremote: Counting objects:  69% (30/43)\u001b[K\rremote: Counting objects:  72% (31/43)\u001b[K\rremote: Counting objects:  74% (32/43)\u001b[K\rremote: Counting objects:  76% (33/43)\u001b[K\rremote: Counting objects:  79% (34/43)\u001b[K\rremote: Counting objects:  81% (35/43)\u001b[K\rremote: Counting objects:  83% (36/43)\u001b[K\rremote: Counting objects:  86% (37/43)\u001b[K\rremote: Counting objects:  88% (38/43)\u001b[K\rremote: Counting objects:  90% (39/43)\u001b[K\rremote: Counting objects:  93% (40/43)\u001b[K\rremote: Counting objects:  95% (41/43)\u001b[K\rremote: Counting objects:  97% (42/43)\u001b[K\rremote: Counting objects: 100% (43/43)\u001b[K\rremote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 23541 (delta 13), reused 15 (delta 5), pack-reused 23498\u001b[K\n",
            "Receiving objects: 100% (23541/23541), 14.04 MiB | 26.52 MiB/s, done.\n",
            "Resolving deltas: 100% (16649/16649), done.\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.33)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=7952ad091e226c608c3542df86cc3fc7edfc44500b02f0f324393dff4c94eb1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clogoJPGd96Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {\n",
        "    'model_type': 'bert',\n",
        "    'data_dir': container_data_dir,\n",
        "    'model_dir': container_model_dir,\n",
        "    'num_train_epochs': 2,\n",
        "    'do_train': True,\n",
        "    'num_labels': 6,\n",
        "    'learning_rate': 1e-4,\n",
        "    'adam_epsilon': 1e-9\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev9c_AJee2ZV",
        "colab_type": "code",
        "outputId": "7e9a3c03-5c84-4083-b0af-689a7cfe3a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 L3_from_pretrain.py \\\n",
        "--model_type 'bert' \\\n",
        "--data_dir 'drive/My Drive/266_train/L3/L3_Bert_7topics' \\\n",
        "--model_name_or_path 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' \\\n",
        "--model_dir \"drive/My Drive/266_train/test/L3_runs/L1only\" \\\n",
        "--num_train_epochs 8 \\\n",
        "--do_train 'True' \\\n",
        "--num_labels 14 \\\n",
        "--learning_rate 1e-4 \\\n",
        "--adam_epsilon 1e-9"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-04 20:05:07.830792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "04/04/2020 20:05:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Preprocessing Goes here\n",
            "Directory: drive/My Drive/266_train/L3/L3_Bert_7topics\n",
            "   id  ...                                               text\n",
            "0   0  ...        The US doesn’t just sell military arsenals.\n",
            "1   1  ...  Under current plans, the Army will recruit an ...\n",
            "2   2  ...  Whatever label you attach to their strategy, t...\n",
            "3   3  ...  Otherwise, the money will have to be allocated...\n",
            "4   4  ...  Open immigration policies and efforts do not a...\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "Loading BERT tokenizer...\n",
            "04/04/2020 20:05:09 - INFO - transformers.tokenization_utils -   Model name 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/04/2020 20:05:09 - INFO - transformers.tokenization_utils -   Didn't find file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/added_tokens.json. We won't load it.\n",
            "04/04/2020 20:05:09 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/vocab.txt\n",
            "04/04/2020 20:05:09 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/04/2020 20:05:09 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/special_tokens_map.json\n",
            "04/04/2020 20:05:09 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/tokenizer_config.json\n",
            "Creating Attention Masks\n",
            "Created train dataset and loader\n",
            "Preprocessing Goes here\n",
            "Directory: drive/My Drive/266_train/L3/L3_Bert_7topics\n",
            "     id  ...                                               text\n",
            "0  8988  ...            Did I mistakenly use the word “person”?\n",
            "1  8989  ...            In this, I am like most American women.\n",
            "2  8990  ...  Arguing from the Law  Roe v. Wade  Most people...\n",
            "3  8991  ...  The availability of legal abortion has had bro...\n",
            "4  8992  ...  And, if they can pass what they have proposed ...\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "Loading BERT tokenizer...\n",
            "04/04/2020 20:05:14 - INFO - transformers.tokenization_utils -   Model name 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/04/2020 20:05:14 - INFO - transformers.tokenization_utils -   Didn't find file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/added_tokens.json. We won't load it.\n",
            "04/04/2020 20:05:14 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/vocab.txt\n",
            "04/04/2020 20:05:14 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/04/2020 20:05:14 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/special_tokens_map.json\n",
            "04/04/2020 20:05:14 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/tokenizer_config.json\n",
            "Creating Attention Masks\n",
            "Created dev dataset and loader\n",
            "04/04/2020 20:05:15 - INFO - transformers.configuration_utils -   loading configuration file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/config.json\n",
            "04/04/2020 20:05:15 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 14,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/04/2020 20:05:15 - INFO - transformers.modeling_utils -   loading weights file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/pytorch_model.bin\n",
            "04/04/2020 20:05:18 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "04/04/2020 20:05:18 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
            "04/04/2020 20:05:21 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-09, data_dir='drive/My Drive/266_train/L3/L3_Bert_7topics', device=device(type='cuda'), do_eval=False, do_train=True, fp16=False, learning_rate=0.0001, local_rank=-1, model_dir='drive/My Drive/266_train/test/L3_runs/L1only', model_name_or_path='drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4', model_type='bert', n_gpu=1, no_cuda=False, num_labels=14, num_train_epochs=8, per_gpu_train_batch_size=8, seed=101)\n",
            "Training Goes here\n",
            "\n",
            "======== Epoch 1 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    281.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    281.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    281.    Elapsed: 0:00:46.\n",
            "  Batch   160  of    281.    Elapsed: 0:01:02.\n",
            "  Batch   200  of    281.    Elapsed: 0:01:17.\n",
            "  Batch   240  of    281.    Elapsed: 0:01:33.\n",
            "  Batch   280  of    281.    Elapsed: 0:01:48.\n",
            "\n",
            "  F1: 0.52\n",
            "  Average training loss: 1.29\n",
            "  Training epoch took: 0:01:48\n",
            "\n",
            "Running Validation...\n",
            "  F1: 0.61\n",
            "  Accuracy: 0.61\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 2 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    281.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    281.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    281.    Elapsed: 0:00:46.\n",
            "  Batch   160  of    281.    Elapsed: 0:01:02.\n",
            "  Batch   200  of    281.    Elapsed: 0:01:17.\n",
            "  Batch   240  of    281.    Elapsed: 0:01:33.\n",
            "  Batch   280  of    281.    Elapsed: 0:01:48.\n",
            "\n",
            "  F1: 0.70\n",
            "  Average training loss: 0.79\n",
            "  Training epoch took: 0:01:48\n",
            "\n",
            "Running Validation...\n",
            "  F1: 0.69\n",
            "  Accuracy: 0.69\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 3 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    281.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    281.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    281.    Elapsed: 0:00:46.\n",
            "  Batch   160  of    281.    Elapsed: 0:01:02.\n",
            "  Batch   200  of    281.    Elapsed: 0:01:17.\n",
            "  Batch   240  of    281.    Elapsed: 0:01:33.\n",
            "  Batch   280  of    281.    Elapsed: 0:01:48.\n",
            "\n",
            "  F1: 0.85\n",
            "  Average training loss: 0.43\n",
            "  Training epoch took: 0:01:48\n",
            "\n",
            "Running Validation...\n",
            "  F1: 0.70\n",
            "  Accuracy: 0.70\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    281.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    281.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    281.    Elapsed: 0:00:46.\n",
            "  Batch   160  of    281.    Elapsed: 0:01:02.\n",
            "  Batch   200  of    281.    Elapsed: 0:01:17.\n",
            "  Batch   240  of    281.    Elapsed: 0:01:33.\n",
            "  Batch   280  of    281.    Elapsed: 0:01:48.\n",
            "\n",
            "  F1: 0.93\n",
            "  Average training loss: 0.21\n",
            "  Training epoch took: 0:01:48\n",
            "\n",
            "Running Validation...\n",
            "  F1: 0.71\n",
            "  Accuracy: 0.71\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 5 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    281.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    281.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    281.    Elapsed: 0:00:46.\n",
            "  Batch   160  of    281.    Elapsed: 0:01:02.\n",
            "  Batch   200  of    281.    Elapsed: 0:01:17.\n",
            "  Batch   240  of    281.    Elapsed: 0:01:33.\n",
            "  Batch   280  of    281.    Elapsed: 0:01:48.\n",
            "\n",
            "  F1: 0.97\n",
            "  Average training loss: 0.09\n",
            "  Training epoch took: 0:01:48\n",
            "\n",
            "Running Validation...\n",
            "  F1: 0.71\n",
            "  Accuracy: 0.71\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 6 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    281.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    281.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    281.    Elapsed: 0:00:46.\n",
            "  Batch   160  of    281.    Elapsed: 0:01:02.\n",
            "  Batch   200  of    281.    Elapsed: 0:01:17.\n",
            "  Batch   240  of    281.    Elapsed: 0:01:32.\n",
            "  Batch   280  of    281.    Elapsed: 0:01:48.\n",
            "\n",
            "  F1: 0.98\n",
            "  Average training loss: 0.06\n",
            "  Training epoch took: 0:01:48\n",
            "\n",
            "Running Validation...\n",
            "  F1: 0.71\n",
            "  Accuracy: 0.71\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 7 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    281.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    281.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    281.    Elapsed: 0:00:46.\n",
            "  Batch   160  of    281.    Elapsed: 0:01:02.\n",
            "  Batch   200  of    281.    Elapsed: 0:01:17.\n",
            "  Batch   240  of    281.    Elapsed: 0:01:32.\n",
            "  Batch   280  of    281.    Elapsed: 0:01:48.\n",
            "\n",
            "  F1: 0.99\n",
            "  Average training loss: 0.03\n",
            "  Training epoch took: 0:01:48\n",
            "\n",
            "Running Validation...\n",
            "  F1: 0.71\n",
            "  Accuracy: 0.71\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 8 / 8 ========\n",
            "Training...\n",
            "  Batch    40  of    281.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    281.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    281.    Elapsed: 0:00:46.\n",
            "  Batch   160  of    281.    Elapsed: 0:01:01.\n",
            "  Batch   200  of    281.    Elapsed: 0:01:17.\n",
            "  Batch   240  of    281.    Elapsed: 0:01:32.\n",
            "  Batch   280  of    281.    Elapsed: 0:01:48.\n",
            "\n",
            "  F1: 0.99\n",
            "  Average training loss: 0.02\n",
            "  Training epoch took: 0:01:48\n",
            "\n",
            "Running Validation...\n",
            "  F1: 0.72\n",
            "  Accuracy: 0.72\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "F1 Train: [0.517022696929239, 0.7001557632398754, 0.8489096573208724, 0.9315754339118825, 0.9718513573653761, 0.9809746328437917, 0.9872051624388073, 0.9898753894080997]\n",
            "F1 Dev: [0.6146146146146146, 0.6896896896896897, 0.7007007007007007, 0.7117117117117117, 0.7077077077077077, 0.7077077077077077, 0.7137137137137137, 0.7227227227227229]\n",
            "Training complete!\n",
            "saved model path\n",
            "Evaluate on Test Dataset!\n",
            "Num records: 2455\n",
            "Tokenize testing, like training\n",
            "04/04/2020 20:20:18 - INFO - transformers.tokenization_utils -   Model name 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/04/2020 20:20:18 - INFO - transformers.tokenization_utils -   Didn't find file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/added_tokens.json. We won't load it.\n",
            "04/04/2020 20:20:18 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/vocab.txt\n",
            "04/04/2020 20:20:18 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/04/2020 20:20:18 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/special_tokens_map.json\n",
            "04/04/2020 20:20:18 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/tokenizer_config.json\n",
            "Created test dataset and loader\n",
            "All predictions made\n",
            "immigration_stance 0.4964705882352941\n",
            "gun_stance 0.43013100436681223\n",
            "medicare_stance 0.5272045028142589\n",
            "abortion_stance 0.49375\n",
            "free_college_stance 0.5224489795918368\n",
            "spending_stance 0.18292682926829268\n",
            "wealth_tax_stance 0.3967741935483871\n",
            "Total Predictions: 2455\n",
            "Biden : stance on immigration: 0.4964705882352941 .... Relative Importance: 0.17311608961303462\n",
            "Biden :stance on guns: 0.43013100436681223 .... Relative Importance: 0.1865580448065173\n",
            "Biden : stance on medicare: 0.5272045028142589 .... Relative Importance: 0.21710794297352343\n",
            "Biden : stance on abortion: 0.49375 .... Relative Importance: 0.13034623217922606\n",
            "Biden : stance on free college: 0.5224489795918368 .... Relative Importance: 0.09979633401221996\n",
            "Biden : stance on military spending: 0.18292682926829268 .... Relative Importance: 0.06680244399185337\n",
            "Biden : stance on wealth tax: 0.3967741935483871 .... Relative Importance: 0.12627291242362526\n",
            "Evaluate on Test Dataset!\n",
            "Num records: 1381\n",
            "Tokenize testing, like training\n",
            "04/04/2020 20:20:29 - INFO - transformers.tokenization_utils -   Model name 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/04/2020 20:20:29 - INFO - transformers.tokenization_utils -   Didn't find file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/added_tokens.json. We won't load it.\n",
            "04/04/2020 20:20:29 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/vocab.txt\n",
            "04/04/2020 20:20:29 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/04/2020 20:20:29 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/special_tokens_map.json\n",
            "04/04/2020 20:20:29 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/tokenizer_config.json\n",
            "Created test dataset and loader\n",
            "All predictions made\n",
            "immigration_stance 0.5674418604651162\n",
            "gun_stance 0.3356164383561644\n",
            "medicare_stance 0.5945945945945946\n",
            "abortion_stance 0.40909090909090906\n",
            "free_college_stance 0.7705882352941176\n",
            "spending_stance 0.12698412698412698\n",
            "wealth_tax_stance 0.4173913043478261\n",
            "Total Predictions: 1381\n",
            "Sanders : stance on immigration: 0.5674418604651162 .... Relative Importance: 0.1556842867487328\n",
            "Sanders :stance on guns: 0.3356164383561644 .... Relative Importance: 0.1057204923968139\n",
            "Sanders : stance on medicare: 0.5945945945945946 .... Relative Importance: 0.21433743664011587\n",
            "Sanders : stance on abortion: 0.40909090909090906 .... Relative Importance: 0.14337436640115858\n",
            "Sanders : stance on free college: 0.7705882352941176 .... Relative Importance: 0.12309920347574221\n",
            "Sanders : stance on military spending: 0.12698412698412698 .... Relative Importance: 0.09123823316437364\n",
            "Sanders : stance on wealth tax: 0.4173913043478261 .... Relative Importance: 0.166545981173063\n",
            "Evaluate on Test Dataset!\n",
            "Num records: 975\n",
            "Tokenize testing, like training\n",
            "04/04/2020 20:20:35 - INFO - transformers.tokenization_utils -   Model name 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/04/2020 20:20:35 - INFO - transformers.tokenization_utils -   Didn't find file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/added_tokens.json. We won't load it.\n",
            "04/04/2020 20:20:35 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/vocab.txt\n",
            "04/04/2020 20:20:35 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/04/2020 20:20:35 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/special_tokens_map.json\n",
            "04/04/2020 20:20:35 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/tokenizer_config.json\n",
            "Created test dataset and loader\n",
            "All predictions made\n",
            "immigration_stance 0.546875\n",
            "gun_stance 0.22018348623853212\n",
            "medicare_stance 0.6714975845410628\n",
            "abortion_stance 0.29268292682926833\n",
            "free_college_stance 0.6161616161616161\n",
            "spending_stance 0.20689655172413796\n",
            "wealth_tax_stance 0.45077720207253885\n",
            "Total Predictions: 975\n",
            "Warren : stance on immigration: 0.546875 .... Relative Importance: 0.13128205128205128\n",
            "Warren :stance on guns: 0.22018348623853212 .... Relative Importance: 0.1117948717948718\n",
            "Warren : stance on medicare: 0.6714975845410628 .... Relative Importance: 0.2123076923076923\n",
            "Warren : stance on abortion: 0.29268292682926833 .... Relative Importance: 0.12615384615384614\n",
            "Warren : stance on free college: 0.6161616161616161 .... Relative Importance: 0.10153846153846154\n",
            "Warren : stance on military spending: 0.20689655172413796 .... Relative Importance: 0.11897435897435897\n",
            "Warren : stance on wealth tax: 0.45077720207253885 .... Relative Importance: 0.19794871794871796\n",
            "Evaluate on Test Dataset!\n",
            "Num records: 1213\n",
            "Tokenize testing, like training\n",
            "04/04/2020 20:20:39 - INFO - transformers.tokenization_utils -   Model name 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/04/2020 20:20:39 - INFO - transformers.tokenization_utils -   Didn't find file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/added_tokens.json. We won't load it.\n",
            "04/04/2020 20:20:39 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/vocab.txt\n",
            "04/04/2020 20:20:39 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/04/2020 20:20:39 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/special_tokens_map.json\n",
            "04/04/2020 20:20:39 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/tokenizer_config.json\n",
            "Created test dataset and loader\n",
            "All predictions made\n",
            "immigration_stance 0.5388888888888889\n",
            "gun_stance 0.2644628099173554\n",
            "medicare_stance 0.635593220338983\n",
            "abortion_stance 0.5\n",
            "free_college_stance 0.49572649572649574\n",
            "spending_stance 0.2809917355371901\n",
            "wealth_tax_stance 0.41860465116279066\n",
            "Total Predictions: 1213\n",
            "Yang : stance on immigration: 0.5388888888888889 .... Relative Importance: 0.1483924154987634\n",
            "Yang :stance on guns: 0.2644628099173554 .... Relative Importance: 0.09975267930750206\n",
            "Yang : stance on medicare: 0.635593220338983 .... Relative Importance: 0.19455894476504534\n",
            "Yang : stance on abortion: 0.5 .... Relative Importance: 0.1483924154987634\n",
            "Yang : stance on free college: 0.49572649572649574 .... Relative Importance: 0.09645507007419621\n",
            "Yang : stance on military spending: 0.2809917355371901 .... Relative Importance: 0.09975267930750206\n",
            "Yang : stance on wealth tax: 0.41860465116279066 .... Relative Importance: 0.21269579554822754\n",
            "Evaluate on Test Dataset!\n",
            "Num records: 1312\n",
            "Tokenize testing, like training\n",
            "04/04/2020 20:20:44 - INFO - transformers.tokenization_utils -   Model name 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/04/2020 20:20:44 - INFO - transformers.tokenization_utils -   Didn't find file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/added_tokens.json. We won't load it.\n",
            "04/04/2020 20:20:44 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/vocab.txt\n",
            "04/04/2020 20:20:44 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/04/2020 20:20:44 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/special_tokens_map.json\n",
            "04/04/2020 20:20:44 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/tokenizer_config.json\n",
            "Created test dataset and loader\n",
            "All predictions made\n",
            "immigration_stance 0.542016806722689\n",
            "gun_stance 0.38562091503267976\n",
            "medicare_stance 0.6238532110091743\n",
            "abortion_stance 0.3279569892473118\n",
            "free_college_stance 0.6526315789473685\n",
            "spending_stance 0.1826086956521739\n",
            "wealth_tax_stance 0.46875\n",
            "Total Predictions: 1312\n",
            "Buttigieg : stance on immigration: 0.542016806722689 .... Relative Importance: 0.18140243902439024\n",
            "Buttigieg :stance on guns: 0.38562091503267976 .... Relative Importance: 0.11661585365853659\n",
            "Buttigieg : stance on medicare: 0.6238532110091743 .... Relative Importance: 0.16615853658536586\n",
            "Buttigieg : stance on abortion: 0.3279569892473118 .... Relative Importance: 0.14176829268292682\n",
            "Buttigieg : stance on free college: 0.6526315789473685 .... Relative Importance: 0.07240853658536585\n",
            "Buttigieg : stance on military spending: 0.1826086956521739 .... Relative Importance: 0.17530487804878048\n",
            "Buttigieg : stance on wealth tax: 0.46875 .... Relative Importance: 0.14634146341463414\n",
            "Evaluate on Test Dataset!\n",
            "Num records: 1664\n",
            "Tokenize testing, like training\n",
            "04/04/2020 20:20:50 - INFO - transformers.tokenization_utils -   Model name 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/04/2020 20:20:50 - INFO - transformers.tokenization_utils -   Didn't find file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/added_tokens.json. We won't load it.\n",
            "04/04/2020 20:20:50 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/vocab.txt\n",
            "04/04/2020 20:20:50 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/04/2020 20:20:50 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/special_tokens_map.json\n",
            "04/04/2020 20:20:50 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/tokenizer_config.json\n",
            "Created test dataset and loader\n",
            "All predictions made\n",
            "immigration_stance 0.4353312302839117\n",
            "gun_stance 0.43842364532019706\n",
            "medicare_stance 0.6755162241887905\n",
            "abortion_stance 0.49145299145299143\n",
            "free_college_stance 0.6486486486486487\n",
            "spending_stance 0.1864406779661017\n",
            "wealth_tax_stance 0.5401785714285714\n",
            "Total Predictions: 1664\n",
            "Klobuchar : stance on immigration: 0.4353312302839117 .... Relative Importance: 0.19050480769230768\n",
            "Klobuchar :stance on guns: 0.43842364532019706 .... Relative Importance: 0.1219951923076923\n",
            "Klobuchar : stance on medicare: 0.6755162241887905 .... Relative Importance: 0.20372596153846154\n",
            "Klobuchar : stance on abortion: 0.49145299145299143 .... Relative Importance: 0.140625\n",
            "Klobuchar : stance on free college: 0.6486486486486487 .... Relative Importance: 0.06670673076923077\n",
            "Klobuchar : stance on military spending: 0.1864406779661017 .... Relative Importance: 0.14182692307692307\n",
            "Klobuchar : stance on wealth tax: 0.5401785714285714 .... Relative Importance: 0.1346153846153846\n",
            "Evaluate on Test Dataset!\n",
            "Num records: 20354\n",
            "Tokenize testing, like training\n",
            "04/04/2020 20:21:00 - INFO - transformers.tokenization_utils -   Model name 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/04/2020 20:21:00 - INFO - transformers.tokenization_utils -   Didn't find file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/added_tokens.json. We won't load it.\n",
            "04/04/2020 20:21:00 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/vocab.txt\n",
            "04/04/2020 20:21:00 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/04/2020 20:21:00 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/special_tokens_map.json\n",
            "04/04/2020 20:21:00 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/266_train/test/pretrained_l2/7topics_1e-4/tokenizer_config.json\n",
            "Created test dataset and loader\n",
            "All predictions made\n",
            "immigration_stance 0.14659074317418602\n",
            "gun_stance 0.359254498714653\n",
            "medicare_stance 0.673972602739726\n",
            "abortion_stance 0.18703855619360132\n",
            "free_college_stance 0.6580188679245282\n",
            "spending_stance 0.2630876958986329\n",
            "wealth_tax_stance 0.3462522851919561\n",
            "Total Predictions: 20354\n",
            "Trump : stance on immigration: 0.14659074317418602 .... Relative Importance: 0.33649405522256065\n",
            "Trump :stance on guns: 0.359254498714653 .... Relative Importance: 0.2293406701385477\n",
            "Trump : stance on medicare: 0.673972602739726 .... Relative Importance: 0.07173037240837182\n",
            "Trump : stance on abortion: 0.18703855619360132 .... Relative Importance: 0.05988994792178442\n",
            "Trump : stance on free college: 0.6580188679245282 .... Relative Importance: 0.020831286233664145\n",
            "Trump : stance on military spending: 0.2630876958986329 .... Relative Importance: 0.1473420457895254\n",
            "Trump : stance on wealth tax: 0.3462522851919561 .... Relative Importance: 0.13437162228554583\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}